# Troubleshooting Log - Smart Grid MARL Project

This document tracks all issues encountered during setup and their solutions.

---

## Issue 1: Python Version Incompatibility

**Problem**: Ray RLlib doesn't support Python 3.13  
**Attempted**: Initial installation with Python 3.13.1  
**Error**: `No matching distribution found for ray`  
**Solution**: Installed Python 3.11.1 and created venv with `py -3.11 -m venv venv`  
**Status**: ‚úÖ RESOLVED

---

## Issue 2: PowerShell Execution Policy

**Problem**: Virtual environment activation script blocked  
**Error**: `cannot be loaded because running scripts is disabled on this system`  
**Solution**: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser -Force`  
**Status**: ‚úÖ RESOLVED

---

## Issue 3: Ray RLlib API Deprecation - `.rollouts()`

**Problem**: Ray 2.52.1 deprecated `.rollouts()` method  
**Error**: `ValueError: rollouts has been deprecated. Use AlgorithmConfig.env_runners(..) instead`  
**Solution**: Changed `.rollouts()` to `.env_runners()` in training script  
**Status**: ‚úÖ RESOLVED

---

## Issue 4: Ray RLlib Training Parameters

**Problem**: Parameter name changes in new Ray version  
**Error**: `TypeError: AlgorithmConfig.training() got an unexpected keyword argument 'sgd_minibatch_size'`  
**Attempted Solutions**:
1. Changed `sgd_minibatch_size` ‚Üí `minibatch_size`
2. Changed `train_batch_size` ‚Üí `train_batch_size_per_learner`
3. Changed `clip_param` ‚Üí `vf_clip_param`
**Status**: ‚ö†Ô∏è PARTIALLY RESOLVED (conflicts with API stack settings)

---

## Issue 5: PyTorch Not Installed

**Problem**: PyTorch wasn't found by Ray  
**Error**: `ImportError: PyTorch was specified as the framework to use... However, no installation was found`  
**Solution**: `pip install torch==2.1.0`  
**Status**: ‚úÖ RESOLVED

---

## Issue 6: NumPy Version Conflicts

**Problem**: NumPy 2.3.5 incompatible with PyTorch 2.1.0  
**Error**: `A module that was compiled using NumPy 1.x cannot be run in NumPy 2.3.5`  
**Attempted**: Downgraded to NumPy 1.24.3  
**New Error**: SciPy required NumPy >=1.25.2  
**Final Solution**: Installed NumPy 1.26.4 (compatible with both)  
**Status**: ‚úÖ RESOLVED

---

## Issue 7: Policy Mapping Function Signature

**Problem**: Ray's new API changed policy_mapping_fn signature  
**Error**: `TypeError: train_smart_grid_agents.<locals>.<lambda>() missing 1 required positional argument: 'worker'`  
**Solution**: Changed `lambda agent_id, episode, worker, **kwargs` ‚Üí `lambda agent_id, episode, **kwargs`  
**Status**: ‚úÖ RESOLVED

---

## Issue 8: PettingZoo Property Conflict

**Problem**: PettingZoo's `ParallelEnv` has `num_agents` as read-only property  
**Error**: `property 'num_agents' of 'SmartGridEnv' object has no setter`  
**Solution**: Changed `self.num_agents` ‚Üí `self._num_agents` and used `len(self.possible_agents)` instead  
**Status**: ‚úÖ RESOLVED

---

## Issue 9: Checkpoint Save Path

**Problem**: Ray requires absolute paths for checkpoints  
**Error**: `pyarrow.lib.ArrowInvalid: URI has empty scheme: 'backend/models/saved_models/best_model'`  
**Solution**: Used `os.path.abspath()` for checkpoint paths  
**Status**: ‚úÖ RESOLVED

---

## Issue 10: Zero Episodes Collected (NEW API STACK)

**Problem**: With new Ray API stack enabled, no episodes being collected  
**Symptoms**: 
- `Episodes Total: 0`
- `Timesteps Total: 0`
- `Mean Reward: 0.00`
**Attempted**: Disabled new API stack with `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`  
**Status**: ‚ö†Ô∏è IN PROGRESS

---

## Issue 11: Legacy API vs New API Conflict

**Problem**: Circular dependency between API versions  
**Scenario**:
- With NEW API enabled: 0 episodes collected (environment doesn't work)
- With NEW API disabled: Old parameter names don't work
**Current State**: Need to find compatible parameter combination  
**Status**: üî¥ ACTIVE ISSUE

---

## Current Blockers

### Primary Issue: Ray RLlib 2.52.1 Compatibility

**The Core Problem**:
Ray version 2.52.1 is in transition between old and new API. Neither works perfectly with our setup:

| Configuration | Result |
|---------------|--------|
| New API Stack ON + New Parameters | 0 episodes collected |
| Old API Stack OFF + Old Parameters | Parameters deprecated/removed |
| Old API Stack OFF + New Parameters | Parameter errors |

**What We're Using**:
- Python 3.11.1
- Ray RLlib 2.52.1
- PettingZoo 1.24.3
- PyTorch 2.1.0
- NumPy 1.26.4

**Next Steps to Try**:
1. ‚úÖ Use simplified parameter set with OLD API stack disabled
2. ‚è≥ Try minimal configuration that's known to work
3. ‚è≥ Consider downgrading Ray to more stable version (2.7.x or 2.9.x)
4. ‚è≥ Check if PettingZoo wrapper needs updating for new Ray API

---

## Summary Statistics

- **Total Issues Encountered**: 11
- **Fully Resolved**: 9
- **In Progress**: 2
- **Time Spent Debugging**: ~2 hours
- **Main Challenge**: Ray RLlib API transition compatibility

---

## Lessons Learned

1. **Python Version Matters**: Always check framework compatibility with Python version
2. **Lock Dependencies**: Use `requirements.lock` for exact versions
3. **API Transitions Are Hard**: Major framework updates can break everything
4. **Windows Adds Complexity**: Execution policies, path handling, etc.
5. **Ray is Powerful but Complex**: Great for distributed RL but steep learning curve

---

## Working Components

Despite training issues, these work perfectly:
- ‚úÖ Custom PettingZoo smart grid environment
- ‚úÖ FastAPI backend with WebSocket
- ‚úÖ Next.js frontend with D3.js visualization
- ‚úÖ Real-time simulation with random agents
- ‚úÖ Professional UI/UX

The system is **90% functional** - only PPO training loop needs fixing!

---

**Last Updated**: December 10, 2024  
**Status**: Active debugging of Ray RLlib training configuration



